#!/usr/bin/env python3
"""
import_shamela.py
Import des fatwas depuis les fichiers Archive .mdb de Shamela
via mdb-export (mdbtools) ‚Üí PostgreSQL

‚ö†Ô∏è ZONE QUASI-SACR√âE : champ `nass` (texte arabe) jamais modifi√©
"""

import csv
import hashlib
import io
import json
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import Optional

import psycopg2
import psycopg2.extras

# ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DB_URL = "postgresql://islampc@localhost:5432/saas_islam"
EXTRACTED_DIR = Path("/tmp/shamela-bok/extracted")
HASHES_FILE = Path("/Users/islampc/.openclaw/workspace/saas-islam/database/integrity/fatwa-hashes.json")
CHECKPOINT_FILE = Path("/tmp/shamela-import-checkpoint.json")

# ‚îÄ‚îÄ Livres cibles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TARGET_BOOKS = [
    {"bkid": 21537, "archive": 4, "madhab": "salafi",  "era": "contemporain",
     "scholar_ar": "ÿπÿ®ÿØ ÿßŸÑÿπÿ≤Ÿäÿ≤ ÿ®ŸÜ ÿπÿ®ÿØ ÿßŸÑŸÑŸá ÿ®ŸÜ ÿ®ÿßÿ≤", "scholar_fr": "Ibn Baz",
     "title_ar": "ŸÖÿ¨ŸÖŸàÿπ ŸÅÿ™ÿßŸàŸâ ÿßÿ®ŸÜ ÿ®ÿßÿ≤", "title_fr": "Majmu' Fatawa Ibn Baz", "vols": 30},

    {"bkid": 12293, "archive": 5, "madhab": "salafi",  "era": "contemporain",
     "scholar_ar": "ŸÖÿ≠ŸÖÿØ ÿ®ŸÜ ÿµÿßŸÑÿ≠ ÿßŸÑÿπÿ´ŸäŸÖŸäŸÜ", "scholar_fr": "Ibn Uthaymin",
     "title_ar": "ŸÖÿ¨ŸÖŸàÿπ ŸÅÿ™ÿßŸàŸâ Ÿàÿ±ÿ≥ÿßÿ¶ŸÑ ÿßŸÑÿπÿ´ŸäŸÖŸäŸÜ", "title_fr": "Majmu' Fatawa Ibn Uthaymin", "vols": 26},

    {"bkid":  8381, "archive": 6, "madhab": "salafi",  "era": "contemporain",
     "scholar_ar": "ÿßŸÑŸÑÿ¨ŸÜÿ© ÿßŸÑÿØÿßÿ¶ŸÖÿ© ŸÑŸÑÿ®ÿ≠Ÿàÿ´ ÿßŸÑÿπŸÑŸÖŸäÿ© ŸàÿßŸÑÿ•ŸÅÿ™ÿßÿ°", "scholar_fr": "Lajnah Ad-Da'ima",
     "title_ar": "ŸÅÿ™ÿßŸàŸâ ÿßŸÑŸÑÿ¨ŸÜÿ© ÿßŸÑÿØÿßÿ¶ŸÖÿ© - 1", "title_fr": "Fatawa Lajnah - Vol.1-11", "vols": 11},

    {"bkid": 21772, "archive": 6, "madhab": "salafi",  "era": "contemporain",
     "scholar_ar": "ÿßŸÑŸÑÿ¨ŸÜÿ© ÿßŸÑÿØÿßÿ¶ŸÖÿ© ŸÑŸÑÿ®ÿ≠Ÿàÿ´ ÿßŸÑÿπŸÑŸÖŸäÿ© ŸàÿßŸÑÿ•ŸÅÿ™ÿßÿ°", "scholar_fr": "Lajnah Ad-Da'ima",
     "title_ar": "ŸÅÿ™ÿßŸàŸâ ÿßŸÑŸÑÿ¨ŸÜÿ© ÿßŸÑÿØÿßÿ¶ŸÖÿ© - 2", "title_fr": "Fatawa Lajnah - Vol.12-26", "vols": 15},

    {"bkid":  9690, "archive": 6, "madhab": "hanbali", "era": "classique",
     "scholar_ar": "ÿ£ÿ≠ŸÖÿØ ÿ®ŸÜ ÿπÿ®ÿØ ÿßŸÑÿ≠ŸÑŸäŸÖ ÿßÿ®ŸÜ ÿ™ŸäŸÖŸäÿ©", "scholar_fr": "Ibn Taymiyyah",
     "title_ar": "ÿßŸÑŸÅÿ™ÿßŸàŸâ ÿßŸÑŸÉÿ®ÿ±Ÿâ ŸÑÿßÿ®ŸÜ ÿ™ŸäŸÖŸäÿ©", "title_fr": "Al-Fatawa al-Kubra - Ibn Taymiyyah", "vols": 6},

    {"bkid": 21640, "archive": 6, "madhab": "hanafi",  "era": "classique",
     "scholar_ar": "ŸÑÿ¨ŸÜÿ© ÿπŸÑŸÖÿßÿ° ÿ®ÿ±ÿ¶ÿßÿ≥ÿ© ŸÜÿ∏ÿßŸÖ ÿßŸÑÿØŸäŸÜ ÿßŸÑÿ®ÿ±ŸáÿßŸÜÿ®Ÿàÿ±Ÿä", "scholar_fr": "Savants hanafites (Hindiyya)",
     "title_ar": "ÿßŸÑŸÅÿ™ÿßŸàŸâ ÿßŸÑŸáŸÜÿØŸäÿ© (ÿßŸÑÿπÿßŸÑŸÖŸÉŸäÿ±Ÿäÿ©)", "title_fr": "Al-Fatawa al-Hindiyya", "vols": 6},

    {"bkid": 11496, "archive": 7, "madhab": "hanbali", "era": "classique",
     "scholar_ar": "ŸÖÿ≠ŸÖÿØ ÿ®ŸÜ ÿ£ÿ®Ÿä ÿ®ŸÉÿ± ÿßÿ®ŸÜ ŸÇŸäŸÖ ÿßŸÑÿ¨Ÿàÿ≤Ÿäÿ©", "scholar_fr": "Ibn al-Qayyim al-Jawziyya",
     "title_ar": "ÿ•ÿπŸÑÿßŸÖ ÿßŸÑŸÖŸàŸÇÿπŸäŸÜ ÿπŸÜ ÿ±ÿ® ÿßŸÑÿπÿßŸÑŸÖŸäŸÜ", "title_fr": "I'lam al-Muwaqqi'in", "vols": 4},

    {"bkid": 21628, "archive": 3, "madhab": "shafii",  "era": "classique",
     "scholar_ar": "ÿ£ÿ≠ŸÖÿØ ÿ®ŸÜ ŸÖÿ≠ŸÖÿØ ÿßÿ®ŸÜ ÿ≠ÿ¨ÿ± ÿßŸÑŸáŸäÿ´ŸÖŸä", "scholar_fr": "Ibn Hajar al-Haytami",
     "title_ar": "ÿßŸÑŸÅÿ™ÿßŸàŸâ ÿßŸÑŸÅŸÇŸáŸäÿ© ÿßŸÑŸÉÿ®ÿ±Ÿâ", "title_fr": "Al-Fatawa al-Fiqhiyya al-Kubra", "vols": 4},

    {"bkid": 21623, "archive": 3, "madhab": "shafii",  "era": "classique",
     "scholar_ar": "ÿ¥ŸÖÿ≥ ÿßŸÑÿØŸäŸÜ ÿßŸÑÿ±ŸÖŸÑŸä", "scholar_fr": "Shams al-Din al-Ramli",
     "title_ar": "ŸÅÿ™ÿßŸàŸâ ÿßŸÑÿ±ŸÖŸÑŸä", "title_fr": "Fatawa al-Ramli", "vols": 4},

    {"bkid": 11498, "archive": 3, "madhab": "shafii",  "era": "classique",
     "scholar_ar": "ÿ™ŸÇŸä ÿßŸÑÿØŸäŸÜ ÿßŸÑÿ≥ÿ®ŸÉŸä", "scholar_fr": "Taqi al-Din al-Subki",
     "title_ar": "ŸÅÿ™ÿßŸàŸâ ÿßŸÑÿ≥ÿ®ŸÉŸä", "title_fr": "Fatawa al-Subki", "vols": 2},
]

# ‚îÄ‚îÄ Classificateur de domaine ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import re
DOMAIN_MAP = [
    (re.compile(r'ÿ∑Ÿáÿßÿ±ÿ©|Ÿàÿ∂Ÿàÿ°|ÿ∫ÿ≥ŸÑ|ÿ™ŸäŸÖŸÖ|ŸÜÿ¨ÿßÿ≥ÿ©|ÿ≠Ÿäÿ∂|ÿ¨ŸÜÿßÿ®ÿ©'), 'purification-taharah'),
    (re.compile(r'ÿµŸÑÿßÿ©|ÿµŸÑŸàÿßÿ™|ÿ¨ŸÖÿπÿ©|ÿ£ÿ∞ÿßŸÜ|ÿ•ŸÖÿßŸÖÿ©|ŸÇÿ®ŸÑÿ©|ÿ≥ÿ¨ŸàÿØ|ÿ±ŸÉŸàÿπ'), 'priere-salat'),
    (re.compile(r'ÿ≤ŸÉÿßÿ©|ÿµÿØŸÇÿ©|ŸÜÿµÿßÿ®|ÿπÿ¥ÿ±'), 'zakat'),
    (re.compile(r'ÿµŸäÿßŸÖ|ÿµŸàŸÖ|ÿ±ŸÖÿ∂ÿßŸÜ|ÿ•ŸÅÿ∑ÿßÿ±|ÿ≥ÿ≠Ÿàÿ±|ÿßÿπÿ™ŸÉÿßŸÅ'), 'jeune-siyam'),
    (re.compile(r'ÿ≠ÿ¨|ÿπŸÖÿ±ÿ©|ÿ•ÿ≠ÿ±ÿßŸÖ|ÿ∑ŸàÿßŸÅ|ÿ≥ÿπŸä|ÿ≠ÿ±ŸÖ|ŸÖŸÉÿ©|ŸÖŸÜŸâ|ÿπÿ±ŸÅÿ©'), 'hajj-umrah'),
    (re.compile(r'ŸÜŸÉÿßÿ≠|ÿ≤Ÿàÿßÿ¨|ÿÆÿ∑ÿ®ÿ©|ŸÖŸáÿ±|ŸàŸÑÿßŸäÿ©|ÿ≤Ÿàÿ¨ÿ©|ÿ≤Ÿàÿ¨'), 'mariage-nikah'),
    (re.compile(r'ÿ∑ŸÑÿßŸÇ|ÿÆŸÑÿπ|ŸÅÿ≥ÿÆ|ÿπÿØÿ©|ÿ±ÿ¨ÿπÿ©|ÿ•ŸäŸÑÿßÿ°|ÿ∏Ÿáÿßÿ±|ŸÑÿπÿßŸÜ'), 'divorce-talaq'),
    (re.compile(r'ŸÖŸàÿßÿ±Ÿäÿ´|ŸÖŸäÿ±ÿßÿ´|ŸàÿµŸäÿ©|ÿ™ÿ±ŸÉÿ©|ŸÅÿ±ÿßÿ¶ÿ∂|ÿ•ÿ±ÿ´|Ÿàÿßÿ±ÿ´'), 'heritage-mawaris'),
    (re.compile(r'ÿ®ŸäŸàÿπ|ÿ®Ÿäÿπ|ÿ¥ÿ±ÿßÿ°|ÿ•ÿ¨ÿßÿ±ÿ©|ŸàŸÉÿßŸÑÿ©|ÿ¥ÿ±ŸÉÿ©|ÿ±ŸáŸÜ|ŸÖÿ∂ÿßÿ±ÿ®ÿ©'), 'commerce-muamalat'),
    (re.compile(r'ÿ±ÿ®ÿß|ŸÖÿµÿßÿ±ŸÅ|ÿ®ŸÜŸàŸÉ|ÿ™ÿ£ŸÖŸäŸÜ|ÿ£ÿ≥ŸáŸÖ|ÿµŸÉŸàŸÉ|ŸÖÿ±ÿßÿ®ÿ≠ÿ©'), 'finance-islamique'),
    (re.compile(r'ÿ£ÿ∑ÿπŸÖÿ©|ÿ∞ÿ®ÿßÿ¶ÿ≠|ÿµŸäÿØ|ÿÆŸÖÿ±|ŸÖÿ≥ŸÉÿ±|ÿ≠ŸÑÿßŸÑ|ÿ≠ÿ±ÿßŸÖ'), 'alimentation-atimah'),
    (re.compile(r'ŸÑÿ®ÿßÿ≥|ÿ≤ŸäŸÜÿ©|ÿ≠ÿ¨ÿßÿ®|ÿ∞Ÿáÿ®|ŸÅÿ∂ÿ©|ÿ≠ÿ±Ÿäÿ±|ÿπÿ∑ÿ±'), 'habillement-libs'),
    (re.compile(r'ÿ£ÿÆŸÑÿßŸÇ|ŸÖÿπÿßŸÖŸÑÿ©|ÿ¨Ÿäÿ±ÿßŸÜ|ÿµŸÑÿ©|ÿ±ÿ≠ŸÖ|ŸàÿßŸÑÿØŸäŸÜ|ÿ®ÿ±'), 'relations-sociales'),
    (re.compile(r'ÿπŸÇŸäÿØÿ©|ÿ™Ÿàÿ≠ŸäÿØ|ÿ•ŸäŸÖÿßŸÜ|ÿ¥ÿ±ŸÉ|ÿ®ÿØÿπÿ©|ŸàŸÑÿßÿ°|ÿ®ÿ±ÿßÿ°'), 'aqida-croyance'),
    (re.compile(r'ŸÇÿ±ÿ¢ŸÜ|ÿ™ŸÑÿßŸàÿ©|ÿ™ÿ¨ŸàŸäÿØ|ÿ≠ŸÅÿ∏|ÿ™ŸÅÿ≥Ÿäÿ±|ŸÖÿµÿ≠ŸÅ'), 'coran-lecture'),
    (re.compile(r'ÿ£ÿ∞ŸÉÿßÿ±|ÿØÿπÿßÿ°|ÿ±ŸÇŸäÿ©|ÿ™ÿ≥ÿ®Ÿäÿ≠|ÿßÿ≥ÿ™ÿ∫ŸÅÿßÿ±'), 'invocations-adkar'),
    (re.compile(r'ÿ∑ÿ®|ÿπŸÑÿßÿ¨|ÿØŸàÿßÿ°|ÿ¨ÿ±ÿßÿ≠ÿ©|ÿ™ÿ®ÿ±ÿπ|ÿ£ÿπÿ∂ÿßÿ°|ŸÖÿ±Ÿäÿ∂'), 'medical-sante'),
    (re.compile(r'ÿπŸÖŸÑ|Ÿàÿ∏ŸäŸÅÿ©|ÿ£ÿ¨ÿ±ÿ©|ŸÖŸàÿ∏ŸÅ|ÿ±ÿßÿ™ÿ®|ŸÖŸáŸÜÿ©'), 'travail-emploi'),
    (re.compile(r'ÿ•ŸÜÿ™ÿ±ŸÜÿ™|Ÿáÿßÿ™ŸÅ|ÿ™ŸÑŸÅÿ≤ŸäŸàŸÜ|ÿµŸàÿ±|ŸÅŸäÿØŸäŸà|ÿ™ÿµŸàŸäÿ±|ÿ≠ÿßÿ≥Ÿàÿ®'), 'technologie-moderne'),
    (re.compile(r'ÿ¨ŸáÿßÿØ|ÿØŸÅÿßÿπ|ÿ£ŸÖÿ©|ÿ≥ŸÑÿ∑ÿßŸÜ|ÿ≠ÿßŸÉŸÖ'), 'jihad-defensif'),
]

def classify_domain(text: str) -> str:
    for pattern, domain in DOMAIN_MAP:
        if pattern.search(text):
            return domain
    return 'divers'

def sha256(text: str) -> str:
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

# ‚îÄ‚îÄ mdb-export helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def read_mdb_table(mdb_path: Path, table: str) -> list[dict]:
    """Lit une table MDB via mdb-export, retourne une liste de dicts."""
    result = subprocess.run(
        ['mdb-export', str(mdb_path), table],
        capture_output=True, text=True, encoding='utf-8', errors='replace'
    )
    if result.returncode != 0:
        return []
    reader = csv.DictReader(io.StringIO(result.stdout))
    return list(reader)

# ‚îÄ‚îÄ Cr√©er les tables si n√©cessaires ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CREATE_SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS app.fatwa_scholars (
    id SERIAL PRIMARY KEY,
    name_arabic TEXT NOT NULL,
    name_fr TEXT,
    name_en TEXT,
    madhab TEXT NOT NULL,
    era TEXT NOT NULL,
    death_year TEXT,
    is_deceased BOOLEAN DEFAULT FALSE,
    UNIQUE(name_arabic, madhab)
);

CREATE TABLE IF NOT EXISTS app.fatwa_books (
    id SERIAL PRIMARY KEY,
    title_arabic TEXT NOT NULL,
    title_fr TEXT,
    scholar_id INT REFERENCES app.fatwa_scholars(id),
    madhab TEXT NOT NULL,
    shamela_id TEXT UNIQUE,
    shamela_local_id INT,
    volume_count INT DEFAULT 1
);

CREATE TABLE IF NOT EXISTS app.fatwas (
    id SERIAL PRIMARY KEY,
    shamela_ref TEXT UNIQUE,
    book_id INT NOT NULL REFERENCES app.fatwa_books(id),
    scholar_id INT REFERENCES app.fatwa_scholars(id),
    volume INT,
    page_number INT,
    question_arabic TEXT,
    answer_arabic TEXT NOT NULL,
    question_fr TEXT,
    answer_fr TEXT,
    question_en TEXT,
    answer_en TEXT,
    is_auto_translated_fr BOOLEAN DEFAULT FALSE,
    is_auto_translated_en BOOLEAN DEFAULT FALSE,
    is_verified_fr BOOLEAN DEFAULT FALSE,
    is_verified_en BOOLEAN DEFAULT FALSE,
    madhab TEXT NOT NULL,
    domain TEXT NOT NULL DEFAULT 'divers',
    sub_domain TEXT,
    tags TEXT[] DEFAULT '{}',
    chapter_hint TEXT,
    period TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS fatwas_madhab_idx ON app.fatwas(madhab);
CREATE INDEX IF NOT EXISTS fatwas_domain_idx ON app.fatwas(domain);
CREATE INDEX IF NOT EXISTS fatwas_madhab_domain_idx ON app.fatwas(madhab, domain);
"""

# ‚îÄ‚îÄ Import principal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def import_book(conn, book: dict, hashes: dict, checkpoint: dict) -> int:
    print(f"\nüìö {book['title_ar']}")
    print(f"   Archive: {book['archive']}.mdb | bkid: {book['bkid']}")

    mdb_path = EXTRACTED_DIR / f"{book['archive']}.mdb"
    if not mdb_path.exists():
        print(f"   ‚ùå Archive {book['archive']}.mdb introuvable")
        return 0

    book_table  = f"b{book['bkid']}"
    title_table = f"t{book['bkid']}"

    # V√©rifier que la table existe
    tables_result = subprocess.run(
        ['mdb-tables', str(mdb_path)], capture_output=True, text=True
    )
    available = tables_result.stdout.split()
    if book_table not in available:
        print(f"   ‚ùå Table {book_table} absente dans Archive {book['archive']}.mdb")
        return 0

    # Lire les titres de chapitres (pour classification)
    chapter_map: dict[int, str] = {}
    if title_table in available:
        title_rows = read_mdb_table(mdb_path, title_table)
        for row in title_rows:
            try:
                chapter_map[int(row['id'])] = row.get('tit', '')
            except (ValueError, KeyError):
                pass
    print(f"   Chapitres charg√©s: {len(chapter_map)}")

    # Lire le contenu
    rows = read_mdb_table(mdb_path, book_table)
    print(f"   Entr√©es totales: {len(rows)}")

    if not rows:
        return 0

    cur = conn.cursor()

    # Upsert savant
    cur.execute("""
        INSERT INTO app.fatwa_scholars (name_arabic, name_fr, madhab, era, is_deceased)
        VALUES (%s, %s, %s, %s, %s)
        ON CONFLICT (name_arabic, madhab) DO UPDATE SET name_fr = EXCLUDED.name_fr
        RETURNING id
    """, (book['scholar_ar'], book['scholar_fr'], book['madhab'], book['era'], book['era'] == 'classique'))
    scholar_id = cur.fetchone()[0]

    # Upsert livre
    local_id_str = str(book['bkid'])
    cur.execute("""
        INSERT INTO app.fatwa_books (title_arabic, title_fr, scholar_id, madhab, shamela_id, shamela_local_id, volume_count)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (shamela_id) DO UPDATE SET title_arabic = EXCLUDED.title_arabic
        RETURNING id
    """, (book['title_ar'], book['title_fr'], scholar_id, book['madhab'],
          local_id_str, book['bkid'], book['vols']))
    book_id = cur.fetchone()[0]
    conn.commit()

    # Import des lignes
    imported = skipped = 0
    batch = []
    BATCH_SIZE = 500

    checkpoint_key = str(book['bkid'])
    start_id = checkpoint.get(checkpoint_key, 0)

    for row in rows:
        nass = row.get('nass', '').strip()
        if not nass or len(nass) < 20 or nass == 'ÿµŸÅÿ≠ÿ© ŸÅÿßÿ±ÿ∫ÿ©':
            continue

        try:
            row_id   = int(row.get('id', 0))
            part_val = int(row.get('part', 0)) if row.get('part') else None
            page_val = int(row.get('page', 0)) if row.get('page') else None
        except ValueError:
            continue

        if row_id <= start_id:
            skipped += 1
            continue

        ref = f"sham-{book['bkid']}-{row_id}"
        h = sha256(nass)
        if hashes.get(ref) == h:
            skipped += 1
            continue

        # Domaine depuis le titre du chapitre le plus proche
        chapter = chapter_map.get(row_id, '')
        domain = classify_domain(chapter + ' ' + nass[:300])

        batch.append((
            ref, book_id, scholar_id, part_val, page_val,
            nass,  # ‚ö†Ô∏è answerArabic IMMUABLE
            book['madhab'], domain, chapter[:200] if chapter else None,
            ref, h
        ))

        if len(batch) >= BATCH_SIZE:
            psycopg2.extras.execute_values(cur, """
                INSERT INTO app.fatwas
                  (shamela_ref, book_id, scholar_id, volume, page_number,
                   answer_arabic, madhab, domain, chapter_hint, tags, is_auto_translated_fr)
                VALUES %s
                ON CONFLICT (shamela_ref) DO NOTHING
            """, [(b[0],b[1],b[2],b[3],b[4],b[5],b[6],b[7],b[8],'{}',False) for b in batch])
            conn.commit()
            for b in batch:
                hashes[b[9]] = b[10]
            imported += len(batch)
            batch.clear()
            checkpoint[checkpoint_key] = row_id
            # Sauvegarde checkpoint + hashes
            CHECKPOINT_FILE.write_text(json.dumps(checkpoint, indent=2))
            print(f"\r   ‚Üí {imported:,} import√©es, {skipped:,} ignor√©es...", end='', flush=True)

    # Dernier batch
    if batch:
        psycopg2.extras.execute_values(cur, """
            INSERT INTO app.fatwas
              (shamela_ref, book_id, scholar_id, volume, page_number,
               answer_arabic, madhab, domain, chapter_hint, tags, is_auto_translated_fr)
            VALUES %s
            ON CONFLICT (shamela_ref) DO NOTHING
        """, [(b[0],b[1],b[2],b[3],b[4],b[5],b[6],b[7],b[8],'{}',False) for b in batch])
        conn.commit()
        for b in batch:
            hashes[b[9]] = b[10]
        imported += len(batch)

    # Sauvegardes finales
    HASHES_FILE.parent.mkdir(parents=True, exist_ok=True)
    HASHES_FILE.write_text(json.dumps(hashes, indent=2))
    checkpoint[checkpoint_key] = 999999999
    CHECKPOINT_FILE.write_text(json.dumps(checkpoint, indent=2))

    cur.close()
    print(f"\n   ‚úÖ {imported:,} fatwas import√©es | {skipped:,} ignor√©es/d√©j√† pr√©sentes")
    return imported

# ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main():
    print("üåô NoorApp ‚Äî Import Fatwas Shamela (MDB ‚Üí PostgreSQL)")
    print("‚ö†Ô∏è  Texte arabe (nass/answer_arabic) IMMUABLE ‚Äî jamais modifi√©\n")

    conn = psycopg2.connect(DB_URL)
    conn.autocommit = False
    cur = conn.cursor()

    # Cr√©er le sch√©ma / tables si inexistantes
    print("üìê Cr√©ation des tables si n√©cessaire...")
    cur.execute(CREATE_SCHEMA_SQL)
    conn.commit()
    cur.close()
    print("   ‚úÖ Sch√©ma pr√™t\n")

    # Charger checkpoint et hashes
    checkpoint: dict = json.loads(CHECKPOINT_FILE.read_text()) if CHECKPOINT_FILE.exists() else {}
    hashes: dict = json.loads(HASHES_FILE.read_text()) if HASHES_FILE.exists() else {}

    report = {}
    total = 0

    for book in TARGET_BOOKS:
        ck = str(book['bkid'])
        if checkpoint.get(ck) == 999999999:
            print(f"\n‚è© {book['title_ar']} ‚Äî d√©j√† import√© (checkpoint)")
            continue
        try:
            count = import_book(conn, book, hashes, checkpoint)
            report[book['title_ar']] = count
            total += count
        except Exception as e:
            print(f"\n‚ùå Erreur sur {book['title_ar']}: {e}")
            conn.rollback()

    conn.close()

    print("\n\n" + "="*60)
    print("üìä RAPPORT FINAL ‚Äî Import Fatwas Shamela")
    print("="*60)
    for title, count in report.items():
        print(f"  {title[:50]:50} : {count:>8,}")
    print(f"\n  {'TOTAL':50} : {total:>8,}")
    print("="*60)
    print("\n‚ö†Ô∏è  Rappel : toutes les fatwas sont is_auto_translated_fr=FALSE")
    print("   La traduction FR sera faite via pipeline DeepL s√©par√©")

if __name__ == '__main__':
    main()
